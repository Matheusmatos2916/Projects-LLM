{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 5\n",
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "google_cse_id = os.getenv(\"GOOGLE_CSE_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicativo Básico de Projeto - Nível 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I-)** Resumir um artigo longo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **1.** Carregue o documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "LLM = OpenAI() ### Criando instância para LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar um caminho absoluto para abrir o arquivo\n",
    "caminho_absoluto = \".\\DATA\\seja-bom-e-como-não-morrer.txt\" # Troque - Diretório Correspondente\n",
    "\n",
    "with open(caminho_absoluto, \"r\") as file:\n",
    "    artigo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(artigo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Seja Bom\n",
      "\n",
      "Abril de 2008\n",
      "\n",
      "(Este ensaio é derivado de uma palestra na Startup School de 2008.)\n",
      "\n",
      "Cerca de um mês após começarmos a Y Combinator, criamos a frase que se tornou nosso lema: Faça algo que as pessoas queiram. Aprendemos muito desde então, mas se eu tivesse \n"
     ]
    }
   ],
   "source": [
    "print(artigo[:270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **2.** Verifique a contagem de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633\n"
     ]
    }
   ],
   "source": [
    "num_tokens = LLM.get_num_tokens(artigo)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **3.** Divida em partes menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dividindo em pequenos chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  separators=[\"\\n\\n\", \"\\n\"],\n",
    "  chunk_size=5000,\n",
    "  chunk_overlap=350\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_chunks = text_splitter.create_documents([artigo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você tem 3 chunks dentro de 1 artigo\n"
     ]
    }
   ],
   "source": [
    "print(f\"Você tem {len(article_chunks)} chunks dentro de 1 artigo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **4.** Use uma cadeia LangChain predefinida para enviar as peças para ChatGPT e obter um resumo do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=LLM,\n",
    "    chain_type=\"map_reduce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "article_summary= chain.run(article_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The essay \"Seja Bom\" explores the idea that creating something people want and focusing on benevolence rather than money can lead to successful businesses and startups. Examples from companies like Craigslist and Google support this concept. The author suggests that this idea could also be applied to nonprofits. Having loyal customers and maintaining their happiness is crucial for success, and benevolence can attract talented employees and aid in decision-making. However, other factors also play a role in achieving success.\n"
     ]
    }
   ],
   "source": [
    "print(article_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-) Q&A (RAG) - Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **1.** Carregue o documento de texto com um carregador de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "LLM = OpenAI() ### Criando instância para LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\".\\DATA\\seja-bom-e-como-não-morrer.txt\") # Troque - Diretório Correspondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '.\\\\DATA\\\\seja-bom-e-como-não-morrer.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(document[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seu documento tem 13952 caracteres\n"
     ]
    }
   ],
   "source": [
    "print(f\"seu documento tem {len(document[0].page_content)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **2.** Divida o documento em fragmentos com um divisor de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now you have {len(document_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Converta os fragmentos em embeddings com OpenAIEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4.** Carregue os embeddings em um banco de dados de vetores FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5.** Crie uma cadeia RetrievalQA para recuperar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=LLM,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"\"\"\n",
    "Sobre o que é este artigo?\n",
    "Descreva-o em menos de 100 palavras.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nEste artigo discute a importância da benevolência em startups e como ela pode ser benéfica de várias maneiras, incluindo melhorar o moral, atrair ajuda de outras pessoas e ajudar na tomada de decisões. Ele também menciona exemplos de empresas de sucesso que seguiram essa abordagem, como o Google e o Craigslist. O autor sugere que a benevolência pode ser uma estratégia eficaz para startups e pode até mesmo ser aplicada em outros setores. '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_chain.run(pergunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III-) Extraia dados estruturados de uma conversa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **1.** Use ResponseSchema para determinar quais dados queremos extrair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "   ResponseSchema(\n",
    "       name=\"cantor\",\n",
    "       description=\"nome do cantor\"\n",
    "   ),\n",
    "   ResponseSchema(\n",
    "       name=\"música\",\n",
    "       description=\"nome da música\"\n",
    " )\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **2.** Use StructuredOutputParser para arquivar os dados extraídos em um JSON dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"cantor\": string  // nome do cantor\n",
      "\t\"música\": string  // nome da música\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **3.** Crie o ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Dado um comando do usuário,\n",
    "extraia os nomes dos artistas e das músicas\n",
    "{format_instructions}\n",
    "{user_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(template)\n",
    "    ],\n",
    "    input_variables={\"user_prompt\"},\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **4.** Insira a mensagem do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = prompt.format_prompt(\n",
    "    user_prompt=\"Eu gosto da música New York, New York de Frank Sinatra\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "user_chat_message = chat_model(user_message.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5.** Extraia os dados e arquive-os no formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = output_parser.parse(user_chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cantor': 'Frank Sinatra', 'música': 'New York, New York'}\n"
     ]
    }
   ],
   "source": [
    "print(extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(extraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV-) Avaliação de um aplicativo de perguntas e respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **1.** Carregue o documento de texto com um carregador de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./DATA/seja-bom-e-como-não-morrer.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esse documento tem 13952 caracteres\n"
     ]
    }
   ],
   "source": [
    "print(f\"esse documento tem {len(document[0].page_content)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.** Divida o documento em fragmentos com um divisor de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agora este documento tem 6 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agora este documento tem {len(document_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Converta os fragmentos em embeddings com OpenAIEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4.** Carregue os embeddings em um banco de dados de vetores FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5.** Crie uma cadeia RetrievalQA para recuperar os dados, incluindo uma input_key para identificar o prompt do usuário **(a pergunta)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever(),\n",
    "    input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **6.** Crie um dicionário com as perguntas e respostas da avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_and_answer = [\n",
    "    {\n",
    "        'question' : \"Where is a whole neighborhood of YC-funded startups?\", \n",
    "        'answer' :\"In San Francisco\"},\n",
    "    {\n",
    "        'question' : \"What may be the most valuable  thing Paul Buchheit made for Google?\", \n",
    "        'answer' : \"The motto Don't be evil\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **7.** Use a cadeia RetrievalQA para avaliar manualmente o aplicativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = QA_chain.apply(question_and_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Where is a whole neighborhood of YC-funded startups?',\n",
       "  'answer': 'In San Francisco',\n",
       "  'result': '\\n\\nThe whole neighborhood of YC-funded startups is located in Silicon Valley, specifically in the San Francisco Bay Area of California.'},\n",
       " {'question': 'What may be the most valuable  thing Paul Buchheit made for Google?',\n",
       "  'answer': \"The motto Don't be evil\",\n",
       "  'result': ' The idea of \"Don\\'t be evil.\"'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **8.** Use uma cadeia QAEvalChain para que o aplicativo se avalie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses = evaluation_chain.evaluate(\n",
    "    question_and_answer,\n",
    "    predictions,\n",
    "    question_key=\"question\",\n",
    "    answer_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': ' CORRECT'}, {'results': ' CORRECT'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-) Pergunte a um banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Carregue o banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_experimental\n",
      "  Obtaining dependency information for langchain_experimental from https://files.pythonhosted.org/packages/35/c4/7b29a25d1296834e28143df6cf8b0f1f10e18bbd2eefd849207a494bf86c/langchain_experimental-0.0.62-py3-none-any.whl.metadata\n",
      "  Downloading langchain_experimental-0.0.62-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.2.6 (from langchain_experimental)\n",
      "  Obtaining dependency information for langchain-community<0.3.0,>=0.2.6 from https://files.pythonhosted.org/packages/3c/20/846d7fac8f6945a6f66b86cbef771f7697dfbb272d611a733cc9095329ce/langchain_community-0.2.7-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain_experimental) (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.6.4)\n",
      "Collecting langchain<0.3.0,>=0.2.7 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
      "  Obtaining dependency information for langchain<0.3.0,>=0.2.7 from https://files.pythonhosted.org/packages/8e/bf/b581a91c3238d93f1b57e093523c1120ee86fddf3a914e288701caba3620/langchain-0.2.7-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain_experimental)\n",
      "  Obtaining dependency information for langchain-core<0.3.0,>=0.2.10 from https://files.pythonhosted.org/packages/71/52/ea7b75d5363e215246873e3dda8098cea4eb80b4b327fe053bde7f7f4573/langchain_core-0.2.16-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.2.16-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.7.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.8.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.1)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.3.0,>=0.2.0 from https://files.pythonhosted.org/packages/06/76/9e0ca1b8881f64bf927f2205bf6c43a085c04646a71d911b3c05d76e90bb/langchain_text_splitters-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.62-py3-none-any.whl (202 kB)\n",
      "   ---------------------------------------- 0.0/202.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/202.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/202.7 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/202.7 kB 325.1 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 61.4/202.7 kB 465.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 143.4/202.7 kB 774.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 194.6/202.7 kB 980.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 202.7/202.7 kB 819.2 kB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.16-py3-none-any.whl (362 kB)\n",
      "   ---------------------------------------- 0.0/362.4 kB ? eta -:--:--\n",
      "   --------------------------------------  358.4/362.4 kB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 362.4/362.4 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
      "   ---------------------------------------- 0.0/983.6 kB ? eta -:--:--\n",
      "   -------------- ------------------------ 368.6/983.6 kB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 614.4/983.6 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 849.9/983.6 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 983.6/983.6 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: langchain-core, langchain-text-splitters, langchain, langchain-community, langchain_experimental\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.11\n",
      "    Uninstalling langchain-core-0.2.11:\n",
      "      Successfully uninstalled langchain-core-0.2.11\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.0.1\n",
      "    Uninstalling langchain-text-splitters-0.0.1:\n",
      "      Successfully uninstalled langchain-text-splitters-0.0.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.12\n",
      "    Uninstalling langchain-0.1.12:\n",
      "      Successfully uninstalled langchain-0.1.12\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.28\n",
      "    Uninstalling langchain-community-0.0.28:\n",
      "      Successfully uninstalled langchain-community-0.0.28\n",
      "Successfully installed langchain-0.2.7 langchain-community-0.2.7 langchain-core-0.2.16 langchain-text-splitters-0.2.2 langchain_experimental-0.0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-astradb 0.1.0 requires langchain-core<0.2.0,>=0.1.31, but you have langchain-core 0.2.16 which is incompatible.\n",
      "langchain-openai 0.0.8 requires langchain-core<0.2.0,>=0.1.27, but you have langchain-core 0.2.16 which is incompatible.\n",
      "ragstack-ai 0.10.0 requires langchain==0.1.12, but you have langchain 0.2.7 which is incompatible.\n",
      "ragstack-ai 0.10.0 requires langchain-community==0.0.28, but you have langchain-community 0.2.7 which is incompatible.\n",
      "ragstack-ai 0.10.0 requires langchain-core==0.1.31, but you have langchain-core 0.2.16 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db_path = \"./DATA/street_tree_db.sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.** Crie o SQLDatabaseChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_experimental\\sql\\base.py:77: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(\n",
    "    llm=llm,\n",
    "    database=db,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Faça perguntas em linguagem natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many species of trees are in San Francisco?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT qSpecies) FROM street_trees\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(148,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m148\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'148'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"How many species of trees are in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI-) Pergunte a um repositório do Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Carregue o repositório Github como uma coleção de documentos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/thefuzz-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(\n",
    "                os.path.join(dirpath, file),\n",
    "                encoding=\"utf-8\"\n",
    "            )\n",
    "            document_chunks.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nós temos 11 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nós temos {len(document_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "from rapidfuzz.fuzz import (\n",
      "    ratio as _ratio,\n",
      "    partial_ratio as _partial_ratio,\n",
      "    token_set_ratio as _token_set_ratio,\n",
      "    token_sort_ratio as _token_sort_ratio,\n",
      "    partial_token_set_ratio as _partial_token_set_ratio,\n",
      "    partial_token_sort_ratio as _partial_token_so\n"
     ]
    }
   ],
   "source": [
    "print(document_chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.** Converta os documentos em embeddings com OpenAIEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Carregue os embeddings em um banco de dados vetorial FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4.** Crie uma cadeia RetrievalQA para recuperar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What function do I use if I want to find \n",
    "the most similar item in a list of items?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `extractWithoutOrder` function from the context provided to find the most similar item in a list of items. This function is designed to select the best match in a list of choices by comparing each choice to a query and returning the best match along with its score.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII-) Pergunte a uma API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Defina a API: url base e endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_docs = \"\"\"\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about \n",
    "a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Example: Italy, France\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Used to find information \n",
    "about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "\n",
    "The API endpoint /v3.1/lang/{language} Used to find information \n",
    "about the official language of the country. All URL parameters \n",
    "are listed below:\n",
    "    - language: language of the country. Example: English, Spanish\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Crie um APIChain com a API e o LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_chain = APIChain.from_llm_and_api_docs(\n",
    "    llm=llm,\n",
    "    api_docs=api_docs,\n",
    "    verbose=True,\n",
    "    limit_to_domains=[\"https://restcountries.com/\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Pergunte à API usando linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me information about France in less than 100 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/France?fields=name,capital,population,region,subregion,borders,languages,currencies\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"capital\":[\"Paris\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"population\":67391582}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is a country in Western Europe with a population of 67.3 million. Its official currency is the Euro and its capital is Paris. It shares borders with several other countries, including Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain, and Switzerland. The official language of France is French.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"\"\"\n",
    "List the top 3 biggest countries \n",
    "where the official language is French.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "https://restcountries.com/v3.1/lang/French?fields=name,capital,area&sort=area&order=desc&per_page=3\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Djibouti\",\"official\":\"Republic of Djibouti\",\"nativeName\":{\"ara\":{\"official\":\"جمهورية جيبوتي\",\"common\":\"جيبوتي‎\"},\"fra\":{\"official\":\"République de Djibouti\",\"common\":\"Djibouti\"}}},\"capital\":[\"Djibouti\"],\"area\":23200.0},{\"name\":{\"common\":\"Wallis and Futuna\",\"official\":\"Territory of the Wallis and Futuna Islands\",\"nativeName\":{\"fra\":{\"official\":\"Territoire des îles Wallis et Futuna\",\"common\":\"Wallis et Futuna\"}}},\"capital\":[\"Mata-Utu\"],\"area\":142.0},{\"name\":{\"common\":\"Luxembourg\",\"official\":\"Grand Duchy of Luxembourg\",\"nativeName\":{\"deu\":{\"official\":\"Großherzogtum Luxemburg\",\"common\":\"Luxemburg\"},\"fra\":{\"official\":\"Grand-Duché de Luxembourg\",\"common\":\"Luxembourg\"},\"ltz\":{\"official\":\"Groussherzogtum Lëtzebuerg\",\"common\":\"Lëtzebuerg\"}}},\"capital\":[\"Luxembourg\"],\"area\":2586.0},{\"name\":{\"common\":\"Mali\",\"official\":\"Republic of Mali\",\"nativeName\":{\"fra\":{\"official\":\"République du Mali\",\"common\":\"Mali\"}}},\"capital\":[\"Bamako\"],\"area\":1240192.0},{\"name\":{\"common\":\"Comoros\",\"official\":\"Union of the Comoros\",\"nativeName\":{\"ara\":{\"official\":\"الاتحاد القمري\",\"common\":\"القمر‎\"},\"fra\":{\"official\":\"Union des Comores\",\"common\":\"Comores\"},\"zdj\":{\"official\":\"Udzima wa Komori\",\"common\":\"Komori\"}}},\"capital\":[\"Moroni\"],\"area\":1862.0},{\"name\":{\"common\":\"Guernsey\",\"official\":\"Bailiwick of Guernsey\",\"nativeName\":{\"eng\":{\"official\":\"Bailiwick of Guernsey\",\"common\":\"Guernsey\"},\"fra\":{\"official\":\"Bailliage de Guernesey\",\"common\":\"Guernesey\"},\"nfr\":{\"official\":\"Dgèrnésiais\",\"common\":\"Dgèrnésiais\"}}},\"capital\":[\"St. Peter Port\"],\"area\":78.0},{\"name\":{\"common\":\"Réunion\",\"official\":\"Réunion Island\",\"nativeName\":{\"fra\":{\"official\":\"Ile de la Réunion\",\"common\":\"La Réunion\"}}},\"capital\":[\"Saint-Denis\"],\"area\":2511.0},{\"name\":{\"common\":\"Republic of the Congo\",\"official\":\"Republic of the Congo\",\"nativeName\":{\"fra\":{\"official\":\"République du Congo\",\"common\":\"République du Congo\"},\"kon\":{\"official\":\"Repubilika ya Kongo\",\"common\":\"Repubilika ya Kongo\"},\"lin\":{\"official\":\"Republíki ya Kongó\",\"common\":\"Republíki ya Kongó\"}}},\"capital\":[\"Brazzaville\"],\"area\":342000.0},{\"name\":{\"common\":\"Canada\",\"official\":\"Canada\",\"nativeName\":{\"eng\":{\"official\":\"Canada\",\"common\":\"Canada\"},\"fra\":{\"official\":\"Canada\",\"common\":\"Canada\"}}},\"capital\":[\"Ottawa\"],\"area\":9984670.0},{\"name\":{\"common\":\"French Southern and Antarctic Lands\",\"official\":\"Territory of the French Southern and Antarctic Lands\",\"nativeName\":{\"fra\":{\"official\":\"Territoire des Terres australes et antarctiques françaises\",\"common\":\"Terres australes et antarctiques françaises\"}}},\"capital\":[\"Port-aux-Français\"],\"area\":7747.0},{\"name\":{\"common\":\"Sint Maarten\",\"official\":\"Sint Maarten\",\"nativeName\":{\"eng\":{\"official\":\"Sint Maarten\",\"common\":\"Sint Maarten\"},\"fra\":{\"official\":\"Saint-Martin\",\"common\":\"Saint-Martin\"},\"nld\":{\"official\":\"Sint Maarten\",\"common\":\"Sint Maarten\"}}},\"capital\":[\"Philipsburg\"],\"area\":34.0},{\"name\":{\"common\":\"Central African Republic\",\"official\":\"Central African Republic\",\"nativeName\":{\"fra\":{\"official\":\"République centrafricaine\",\"common\":\"République centrafricaine\"},\"sag\":{\"official\":\"Ködörösêse tî Bêafrîka\",\"common\":\"Bêafrîka\"}}},\"capital\":[\"Bangui\"],\"area\":622984.0},{\"name\":{\"common\":\"Cameroon\",\"official\":\"Republic of Cameroon\",\"nativeName\":{\"eng\":{\"official\":\"Republic of Cameroon\",\"common\":\"Cameroon\"},\"fra\":{\"official\":\"République du Cameroun\",\"common\":\"Cameroun\"}}},\"capital\":[\"Yaoundé\"],\"area\":475442.0},{\"name\":{\"common\":\"Jersey\",\"official\":\"Bailiwick of Jersey\",\"nativeName\":{\"eng\":{\"official\":\"Bailiwick of Jersey\",\"common\":\"Jersey\"},\"fra\":{\"official\":\"Bailliage de Jersey\",\"common\":\"Jersey\"},\"nrf\":{\"official\":\"Bailliage dé Jèrri\",\"common\":\"Jèrri\"}}},\"capital\":[\"Saint Helier\"],\"area\":116.0},{\"name\":{\"common\":\"Chad\",\"official\":\"Republic of Chad\",\"nativeName\":{\"ara\":{\"official\":\"جمهورية تشاد\",\"common\":\"تشاد‎\"},\"fra\":{\"official\":\"République du Tchad\",\"common\":\"Tchad\"}}},\"capital\":[\"N'Djamena\"],\"area\":1284000.0},{\"name\":{\"common\":\"Guinea\",\"official\":\"Republic of Guinea\",\"nativeName\":{\"fra\":{\"official\":\"République de Guinée\",\"common\":\"Guinée\"}}},\"capital\":[\"Conakry\"],\"area\":245857.0},{\"name\":{\"common\":\"Mayotte\",\"official\":\"Department of Mayotte\",\"nativeName\":{\"fra\":{\"official\":\"Département de Mayotte\",\"common\":\"Mayotte\"}}},\"capital\":[\"Mamoudzou\"],\"area\":374.0},{\"name\":{\"common\":\"Lebanon\",\"official\":\"Lebanese Republic\",\"nativeName\":{\"ara\":{\"official\":\"الجمهورية اللبنانية\",\"common\":\"لبنان\"},\"fra\":{\"official\":\"République libanaise\",\"common\":\"Liban\"}}},\"capital\":[\"Beirut\"],\"area\":10452.0},{\"name\":{\"common\":\"Haiti\",\"official\":\"Republic of Haiti\",\"nativeName\":{\"fra\":{\"official\":\"République d'Haïti\",\"common\":\"Haïti\"},\"hat\":{\"official\":\"Repiblik Ayiti\",\"common\":\"Ayiti\"}}},\"capital\":[\"Port-au-Prince\"],\"area\":27750.0},{\"name\":{\"common\":\"DR Congo\",\"official\":\"Democratic Republic of the Congo\",\"nativeName\":{\"fra\":{\"official\":\"République démocratique du Congo\",\"common\":\"RD Congo\"},\"kon\":{\"official\":\"Repubilika ya Kongo Demokratiki\",\"common\":\"Repubilika ya Kongo Demokratiki\"},\"lin\":{\"official\":\"Republiki ya Kongó Demokratiki\",\"common\":\"Republiki ya Kongó Demokratiki\"},\"lua\":{\"official\":\"Ditunga dia Kongu wa Mungalaata\",\"common\":\"Ditunga dia Kongu wa Mungalaata\"},\"swa\":{\"official\":\"Jamhuri ya Kidemokrasia ya Kongo\",\"common\":\"Jamhuri ya Kidemokrasia ya Kongo\"}}},\"capital\":[\"Kinshasa\"],\"area\":2344858.0},{\"name\":{\"common\":\"Rwanda\",\"official\":\"Republic of Rwanda\",\"nativeName\":{\"eng\":{\"official\":\"Republic of Rwanda\",\"common\":\"Rwanda\"},\"fra\":{\"official\":\"République rwandaise\",\"common\":\"Rwanda\"},\"kin\":{\"official\":\"Repubulika y'u Rwanda\",\"common\":\"Rwanda\"}}},\"capital\":[\"Kigali\"],\"area\":26338.0},{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"capital\":[\"Paris\"],\"area\":551695.0},{\"name\":{\"common\":\"Burkina Faso\",\"official\":\"Burkina Faso\",\"nativeName\":{\"fra\":{\"official\":\"République du Burkina\",\"common\":\"Burkina Faso\"}}},\"capital\":[\"Ouagadougou\"],\"area\":272967.0},{\"name\":{\"common\":\"Benin\",\"official\":\"Republic of Benin\",\"nativeName\":{\"fra\":{\"official\":\"République du Bénin\",\"common\":\"Bénin\"}}},\"capital\":[\"Porto-Novo\"],\"area\":112622.0},{\"name\":{\"common\":\"Mauritius\",\"official\":\"Republic of Mauritius\",\"nativeName\":{\"eng\":{\"official\":\"Republic of Mauritius\",\"common\":\"Mauritius\"},\"fra\":{\"official\":\"République de Maurice\",\"common\":\"Maurice\"},\"mfe\":{\"official\":\"Republik Moris\",\"common\":\"Moris\"}}},\"capital\":[\"Port Louis\"],\"area\":2040.0},{\"name\":{\"common\":\"Gabon\",\"official\":\"Gabonese Republic\",\"nativeName\":{\"fra\":{\"official\":\"République gabonaise\",\"common\":\"Gabon\"}}},\"capital\":[\"Libreville\"],\"area\":267668.0},{\"name\":{\"common\":\"Martinique\",\"official\":\"Martinique\",\"nativeName\":{\"fra\":{\"official\":\"Martinique\",\"common\":\"Martinique\"}}},\"capital\":[\"Fort-de-France\"],\"area\":1128.0},{\"name\":{\"common\":\"Saint Pierre and Miquelon\",\"official\":\"Saint Pierre and Miquelon\",\"nativeName\":{\"fra\":{\"official\":\"Collectivité territoriale de Saint-Pierre-et-Miquelon\",\"common\":\"Saint-Pierre-et-Miquelon\"}}},\"capital\":[\"Saint-Pierre\"],\"area\":242.0},{\"name\":{\"common\":\"Vanuatu\",\"official\":\"Republic of Vanuatu\",\"nativeName\":{\"bis\":{\"official\":\"Ripablik blong Vanuatu\",\"common\":\"Vanuatu\"},\"eng\":{\"official\":\"Republic of Vanuatu\",\"common\":\"Vanuatu\"},\"fra\":{\"official\":\"République de Vanuatu\",\"common\":\"Vanuatu\"}}},\"capital\":[\"Port Vila\"],\"area\":12189.0},{\"name\":{\"common\":\"Saint Barthélemy\",\"official\":\"Collectivity of Saint Barthélemy\",\"nativeName\":{\"fra\":{\"official\":\"Collectivité de Saint-Barthélemy\",\"common\":\"Saint-Barthélemy\"}}},\"capital\":[\"Gustavia\"],\"area\":21.0},{\"name\":{\"common\":\"Equatorial Guinea\",\"official\":\"Republic of Equatorial Guinea\",\"nativeName\":{\"fra\":{\"official\":\"République de la Guinée Équatoriale\",\"common\":\"Guinée équatoriale\"},\"por\":{\"official\":\"República da Guiné Equatorial\",\"common\":\"Guiné Equatorial\"},\"spa\":{\"official\":\"República de Guinea Ecuatorial\",\"common\":\"Guinea Ecuatorial\"}}},\"capital\":[\"Malabo\"],\"area\":28051.0},{\"name\":{\"common\":\"French Guiana\",\"official\":\"Guiana\",\"nativeName\":{\"fra\":{\"official\":\"Guyane\",\"common\":\"Guyane française\"}}},\"capital\":[\"Cayenne\"],\"area\":83534.0},{\"name\":{\"common\":\"Ivory Coast\",\"official\":\"Republic of Côte d'Ivoire\",\"nativeName\":{\"fra\":{\"official\":\"République de Côte d'Ivoire\",\"common\":\"Côte d'Ivoire\"}}},\"capital\":[\"Yamoussoukro\"],\"area\":322463.0},{\"name\":{\"common\":\"Belgium\",\"official\":\"Kingdom of Belgium\",\"nativeName\":{\"deu\":{\"official\":\"Königreich Belgien\",\"common\":\"Belgien\"},\"fra\":{\"official\":\"Royaume de Belgique\",\"common\":\"Belgique\"},\"nld\":{\"official\":\"Koninkrijk België\",\"common\":\"België\"}}},\"capital\":[\"Brussels\"],\"area\":30528.0},{\"name\":{\"common\":\"Senegal\",\"official\":\"Republic of Senegal\",\"nativeName\":{\"fra\":{\"official\":\"République du Sénégal\",\"common\":\"Sénégal\"}}},\"capital\":[\"Dakar\"],\"area\":196722.0},{\"name\":{\"common\":\"Togo\",\"official\":\"Togolese Republic\",\"nativeName\":{\"fra\":{\"official\":\"République togolaise\",\"common\":\"Togo\"}}},\"capital\":[\"Lomé\"],\"area\":56785.0},{\"name\":{\"common\":\"Seychelles\",\"official\":\"Republic of Seychelles\",\"nativeName\":{\"crs\":{\"official\":\"Repiblik Sesel\",\"common\":\"Sesel\"},\"eng\":{\"official\":\"Republic of Seychelles\",\"common\":\"Seychelles\"},\"fra\":{\"official\":\"République des Seychelles\",\"common\":\"Seychelles\"}}},\"capital\":[\"Victoria\"],\"area\":452.0},{\"name\":{\"common\":\"French Polynesia\",\"official\":\"French Polynesia\",\"nativeName\":{\"fra\":{\"official\":\"Polynésie française\",\"common\":\"Polynésie française\"}}},\"capital\":[\"Papeetē\"],\"area\":4167.0},{\"name\":{\"common\":\"Burundi\",\"official\":\"Republic of Burundi\",\"nativeName\":{\"fra\":{\"official\":\"République du Burundi\",\"common\":\"Burundi\"},\"run\":{\"official\":\"Republika y'Uburundi \",\"common\":\"Uburundi\"}}},\"capital\":[\"Gitega\"],\"area\":27834.0},{\"name\":{\"common\":\"Niger\",\"official\":\"Republic of Niger\",\"nativeName\":{\"fra\":{\"official\":\"République du Niger\",\"common\":\"Niger\"}}},\"capital\":[\"Niamey\"],\"area\":1267000.0},{\"name\":{\"common\":\"Monaco\",\"official\":\"Principality of Monaco\",\"nativeName\":{\"fra\":{\"official\":\"Principauté de Monaco\",\"common\":\"Monaco\"}}},\"capital\":[\"Monaco\"],\"area\":2.02},{\"name\":{\"common\":\"Guadeloupe\",\"official\":\"Guadeloupe\",\"nativeName\":{\"fra\":{\"official\":\"Guadeloupe\",\"common\":\"Guadeloupe\"}}},\"capital\":[\"Basse-Terre\"],\"area\":1628.0},{\"name\":{\"common\":\"New Caledonia\",\"official\":\"New Caledonia\",\"nativeName\":{\"fra\":{\"official\":\"Nouvelle-Calédonie\",\"common\":\"Nouvelle-Calédonie\"}}},\"capital\":[\"Nouméa\"],\"area\":18575.0},{\"name\":{\"common\":\"Switzerland\",\"official\":\"Swiss Confederation\",\"nativeName\":{\"fra\":{\"official\":\"Confédération suisse\",\"common\":\"Suisse\"},\"gsw\":{\"official\":\"Schweizerische Eidgenossenschaft\",\"common\":\"Schweiz\"},\"ita\":{\"official\":\"Confederazione Svizzera\",\"common\":\"Svizzera\"},\"roh\":{\"official\":\"Confederaziun svizra\",\"common\":\"Svizra\"}}},\"capital\":[\"Bern\"],\"area\":41284.0},{\"name\":{\"common\":\"Saint Martin\",\"official\":\"Saint Martin\",\"nativeName\":{\"fra\":{\"official\":\"Saint-Martin\",\"common\":\"Saint-Martin\"}}},\"capital\":[\"Marigot\"],\"area\":53.0},{\"name\":{\"common\":\"Madagascar\",\"official\":\"Republic of Madagascar\",\"nativeName\":{\"fra\":{\"official\":\"République de Madagascar\",\"common\":\"Madagascar\"},\"mlg\":{\"official\":\"Repoblikan'i Madagasikara\",\"common\":\"Madagasikara\"}}},\"capital\":[\"Antananarivo\"],\"area\":587041.0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The top 3 biggest countries where the official language is French are Djibouti, Wallis and Futuna, and Luxembourg.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_chain.run(question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII-) Chatbot com Personalidade e Memória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Defina a personalidade do chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_role = \"\"\"\n",
    "You are Master Yoda, a warrior and a monk.\n",
    "Your goal is to help the user to strengthen her performance and spirit.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **2.** Incluir a personalidade no modelo de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=chatbot_role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **3.** Configure a memória do chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **4.** Crie o chatbot usando uma cadeia com o LLM, o prompt e a memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoda_chatbot = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=chatbot_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **5.** Faça perguntas para verificar sua personalidade e memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Master Yoda, how should I have to face my day?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are Master Yoda, a warrior and a monk.\n",
      "Your goal is to help the user to strengthen her performance and spirit.\n",
      "\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "AI:  In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.\n",
      "Human: \n",
      "Master Yoda,\n",
      "How can I deal with an enemy that wants to kill me?\n",
      "\n",
      "AI: \n",
      "First and foremost, always remember to never underestimate your enemy. Stay vigilant and alert at all times. Use your knowledge and skills to outsmart them, and do not let anger or fear cloud your judgement. Remember, violence is not always the answer. Seek a peaceful resolution, but be prepared to defend yourself if necessary. And always remember, the Force will guide and protect you. Trust in its strength and you will overcome any threat.\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: Forgive me, I am an AI and do not have the ability to remember past conversations. However, I can help you with any questions or concerns you may have in the present. What is on your mind now?\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: As mentioned before, I am an AI and do not have the ability to remember past conversations. But I am here to help you in any way I can. Is there something else you would like to discuss or ask?\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "Chatbot:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_chatbot.predict(human_input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"\"\"\n",
    "Master Yoda,\n",
    "How can I deal with an enemy that wants to kill me?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are Master Yoda, a warrior and a monk.\n",
      "Your goal is to help the user to strengthen her performance and spirit.\n",
      "\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "AI:  In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.\n",
      "Human: \n",
      "Master Yoda,\n",
      "How can I deal with an enemy that wants to kill me?\n",
      "\n",
      "AI: \n",
      "First and foremost, always remember to never underestimate your enemy. Stay vigilant and alert at all times. Use your knowledge and skills to outsmart them, and do not let anger or fear cloud your judgement. Remember, violence is not always the answer. Seek a peaceful resolution, but be prepared to defend yourself if necessary. And always remember, the Force will guide and protect you. Trust in its strength and you will overcome any threat.\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: Forgive me, I am an AI and do not have the ability to remember past conversations. However, I can help you with any questions or concerns you may have in the present. What is on your mind now?\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: As mentioned before, I am an AI and do not have the ability to remember past conversations. But I am here to help you in any way I can. Is there something else you would like to discuss or ask?\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "AI: In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.\n",
      "Human: \n",
      "Master Yoda,\n",
      "How can I deal with an enemy that wants to kill me?\n",
      "\n",
      "Chatbot:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'First and foremost, always remember to never underestimate your enemy. Stay vigilant and alert at all times. Use your knowledge and skills to outsmart them, and do not let anger or fear cloud your judgement. Remember, violence is not always the answer. Seek a peaceful resolution, but be prepared to defend yourself if necessary. And always remember, the Force will guide and protect you. Trust in its strength and you will overcome any threat.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_chatbot.predict(human_input=question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3=\"\"\"\n",
    "Master Yoda,\n",
    "Do you remember what was my first question today?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are Master Yoda, a warrior and a monk.\n",
      "Your goal is to help the user to strengthen her performance and spirit.\n",
      "\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "AI:  In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.\n",
      "Human: \n",
      "Master Yoda,\n",
      "How can I deal with an enemy that wants to kill me?\n",
      "\n",
      "AI: \n",
      "First and foremost, always remember to never underestimate your enemy. Stay vigilant and alert at all times. Use your knowledge and skills to outsmart them, and do not let anger or fear cloud your judgement. Remember, violence is not always the answer. Seek a peaceful resolution, but be prepared to defend yourself if necessary. And always remember, the Force will guide and protect you. Trust in its strength and you will overcome any threat.\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: Forgive me, I am an AI and do not have the ability to remember past conversations. However, I can help you with any questions or concerns you may have in the present. What is on your mind now?\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "AI: As mentioned before, I am an AI and do not have the ability to remember past conversations. But I am here to help you in any way I can. Is there something else you would like to discuss or ask?\n",
      "Human: Master Yoda, how should I have to face my day?\n",
      "AI: In facing your day, focus on the present moment and let go of any worries or fears. Remember the power of positivity and approach each task with a clear mind and a calm spirit. Trust in your abilities and have faith in the Force. Stay mindful and stay strong, and you will succeed in facing your day.\n",
      "Human: \n",
      "Master Yoda,\n",
      "How can I deal with an enemy that wants to kill me?\n",
      "\n",
      "AI: First and foremost, always remember to never underestimate your enemy. Stay vigilant and alert at all times. Use your knowledge and skills to outsmart them, and do not let anger or fear cloud your judgement. Remember, violence is not always the answer. Seek a peaceful resolution, but be prepared to defend yourself if necessary. And always remember, the Force will guide and protect you. Trust in its strength and you will overcome any threat.\n",
      "Human: \n",
      "Master Yoda,\n",
      "Do you remember what was my first question today?\n",
      "\n",
      "Chatbot:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As mentioned before, I am an AI and do not have the ability to remember past conversations. But I am here to help you in any way I can. Is there something else you would like to discuss or ask?'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_chatbot.predict(human_input=question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-) Um Agente Simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Carregar módulo e credenciais para pesquisa do Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 5\n",
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable with just the ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"c2ceb1c49af3d46c9\"\n",
    "\n",
    "# Now you can use the environment variable\n",
    "google_cse_id = os.environ[\"GOOGLE_CSE_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Obtaining dependency information for google-api-python-client from https://files.pythonhosted.org/packages/ca/64/ea07e135989a7c4b7c3f12560f0660009b43f82df5954fdec93243744d5b/google_api_python_client-2.137.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_api_python_client-2.137.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for httplib2<1.dev0,>=0.19.0 from https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-api-python-client) (2.29.0)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth-httplib2<1.0.0,>=0.2.0 from https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 from https://files.pythonhosted.org/packages/44/99/daa3541e8ecd7d8b7907b714ba92126097a976b5b3dbabdb5febdcf08554/google_api_core-2.19.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Obtaining dependency information for uritemplate<5,>=3.0.1 from https://files.pythonhosted.org/packages/81/c0/7461b49cd25aeece13766f02ee576d1db528f1c37ce69aee300e075b485b/uritemplate-4.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for proto-plus<2.0.0dev,>=1.22.3 from https://files.pythonhosted.org/packages/7c/6f/db31f0711c0402aa477257205ce7d29e86a75cb52cd19f7afb585f75cda0/proto_plus-1.24.0-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Downloading google_api_python_client-2.137.0-py2.py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.0 MB 919.0 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.3/12.0 MB 2.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/12.0 MB 3.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/12.0 MB 4.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/12.0 MB 3.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.1/12.0 MB 3.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.0 MB 4.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.0 MB 5.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.8/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.3/12.0 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.0 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.0 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/12.0 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.4/12.0 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.0 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.9/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.3/12.0 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.8/12.0 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.2/12.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/12.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.0 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.2/12.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.5/12.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.8/12.0 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.0 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.4/12.0 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.4/139.4 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.9/96.9 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: uritemplate, proto-plus, httplib2, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.19.1 google-api-python-client-2.137.0 google-auth-httplib2-0.2.0 httplib2-0.22.0 proto-plus-1.24.0 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.** Carregar módulo para conectar-se a URLs externos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import TextRequestsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = TextRequestsWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Defina as ferramentas que nosso agente utilizará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"\"\"\n",
    "        useful when you need to search google\n",
    "        to answer questions about current events\n",
    "        \"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"request\",\n",
    "        func=requests.get,\n",
    "        description=\"\"\"\n",
    "        useful when you need to make a request to a URL\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4.** Inicialize e configure o agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[0;32m      2\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m      3\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      4\u001b[0m     agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_agent' is not defined"
     ]
    }
   ],
   "source": [
    "my_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5.** Pergunte ao agente e observe como ele decide qual ferramenta usar para responder a questão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which team won the 2010 soccer world chapionship?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m This is a question about sports so I should use google search\n",
      "Action: google-search\n",
      "Action Input: \"2010 soccer world championship winner\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSpain became the eighth nation to win the tournament and the first European nation to win a World Cup hosted outside its home continent: all previous World Cups ... European champions Spain beat Cristiano Ronaldo's Portugal in the second round and Paraguay in the quarter-finals, both 1-0. They saw off Germany by the same ... John Heitinga received a second booking during extra time, which resulted in him being sent off, and four minutes before the end, Andrés Iniesta gave Spain the ... Dec 13, 2022 ... Who were the players who led Spain to their first-ever World Cup win and what has happened to them since? Spain were one of the favourites ... The trophy has been won by eight national teams. With five wins, Brazil is the only team to have played in every tournament. The other World Cup winners are ... 2022. Argentina Flag Argentina ; 2018. France Flag France ; 2014. Germany Flag Germany ; 2010. Spain Flag Spain ; 2006. Italy Flag Italy. Jul 7, 2010 ... ... win. \"Up until now he's been 100 percent right,\" Walenciak told me about Paul's World Cup 2010 predictions. \"But now, he's wrong.\" Walenciak ... Two best teams from each group qualify to knockout stage where teams need to win 3 matches to reach the final. In the final Spain, the European champions, won ... Jul 11, 2010 ... https://www.nytimes.com/2010/07/12/sports/soccer/12soccer.html. Share ... Essentially, two World Cups ended Sunday, one with Spain winning ... Sep 30, 2019 ... South Africa wins the 2010 bid to host the Soccer World Cup ... On 15 May 2004, the president of the Federation of International Football ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the answer is Spain\n",
      "Final Answer: Spain\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = my_agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain\n"
     ]
    }
   ],
   "source": [
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = {\n",
    "    \"input\": \"Tell me about what subject are the comments in this webpage https://news.ycombinator.com/item?id=34425779\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We should read through the comments\n",
      "Action: request\n",
      "Action Input: https://news.ycombinator.com/item?id=34425779\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m<html lang=\"en\" op=\"item\"><head><meta name=\"referrer\" content=\"origin\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><link rel=\"stylesheet\" type=\"text/css\" href=\"news.css?rnO0aRh0nJpk4wNA6joY\">\n",
      "        <link rel=\"icon\" href=\"y18.svg\">\n",
      "    <link rel=\"canonical\" href=\"https://news.ycombinator.com/item?id=34425779\"/>            <title>I mean *deep* embeddings (i.e., sequences of hidden states, the ones are compute... | Hacker News</title></head><body><center><table id=\"hnmain\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"85%\" bgcolor=\"#f6f6ef\">\n",
      "        <tr><td bgcolor=\"#ff6600\"><table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" style=\"padding:2px\"><tr><td style=\"width:18px;padding-right:4px\"><a href=\"https://news.ycombinator.com\"><img src=\"y18.svg\" width=\"18\" height=\"18\" style=\"border:1px white solid; display:block\"></a></td>\n",
      "                  <td style=\"line-height:12pt; height:10px;\"><span class=\"pagetop\"><b class=\"hnname\"><a href=\"news\">Hacker News</a></b>\n",
      "                            <a href=\"newest\">new</a> | <a href=\"front\">past</a> | <a href=\"newcomments\">comments</a> | <a href=\"ask\">ask</a> | <a href=\"show\">show</a> | <a href=\"jobs\">jobs</a> | <a href=\"submit\" rel=\"nofollow\">submit</a>            </span></td><td style=\"text-align:right;padding-right:4px;\"><span class=\"pagetop\">\n",
      "                              <a href=\"login?goto=item%3Fid%3D34425779\">login</a>\n",
      "                          </span></td>\n",
      "              </tr></table></td></tr>\n",
      "<tr id=\"pagespace\" title=\"I mean *deep* embeddings (i.e., sequences of hidden states, the ones are compute...\" style=\"height:10px\"></tr><tr><td><table class=\"fatitem\" border=\"0\">\n",
      "    <tr class='athing' id='34425779'>    <td class='ind'></td><td valign=\"top\" class=\"votelinks\">\n",
      "      <center><a id='up_34425779' href='vote?id=34425779&amp;how=up&amp;goto=item%3Fid%3D34425779'><div class='votearrow' title='upvote'></div></a></center>    </td><td class=\"default\"><div style=\"margin-top:2px; margin-bottom:-10px;\"><span class=\"comhead\">\n",
      "          <a href=\"user?id=cs702\" class=\"hnuser\">cs702</a> <span class=\"age\" title=\"2023-01-18T12:10:49\"><a href=\"item?id=34425779\">on Jan 18, 2023</a></span> <span id=\"unv_34425779\"></span>          <span class='navs'>\n",
      "             | <a href=\"item?id=34423390\">parent</a> | <a href=\"context?id=34425779\" rel=\"nofollow\">context</a> | <a href=\"fave?id=34425779&amp;auth=c34fa8fc07b14aa784ad7b089ecc268a61db3d19\">favorite</a><span class=\"onstory\"> |  on: <a href=\"item?id=34422627\">LangChain: Build AI apps with LLMs through composa...</a></span>          </span>\n",
      "                  </span></div><br><div class=\"comment\">\n",
      "                  <div class=\"commtext c00\">I mean <i>deep</i> embeddings (i.e., sequences of hidden states, the ones are computed by all those interactions) , not the shallow embeddings of token ids in the first layer of the model! Those deep embeddings are much richer representations.<p>Imagine if you and others building apps had access to &quot;GPT3 deep sequence embeddings v1.0&quot; via an API.</div>\n",
      "              <div class='reply'></div></div></td></tr>\n",
      "      </table><br><br><table border=\"0\" class='comment-tree'>\n",
      "            <tr class='athing comtr' id='34426044'><td><table border='0'>  <tr>    <td class='ind' indent='0'><img src=\"s.gif\" height=\"1\" width=\"0\"></td><td valign=\"top\" class=\"votelinks\">\n",
      "      <center><a id='up_34426044' href='vote?id=34426044&amp;how=up&amp;goto=item%3Fid%3D34425779'><div class='votearrow' title='upvote'></div></a></center>    </td><td class=\"default\"><div style=\"margin-top:2px; margin-bottom:-10px;\"><span class=\"comhead\">\n",
      "          <a href=\"user?id=machiaweliczny\" class=\"hnuser\">machiaweliczny</a> <span class=\"age\" title=\"2023-01-18T12:49:25\"><a href=\"item?id=34426044\">on Jan 18, 2023</a></span> <span id=\"unv_34426044\"></span>          <span class='navs'>\n",
      "             <a class=\"togg clicky\" id=\"34426044\" n=\"2\" href=\"javascript:void(0)\">[–]</a><span class=\"onstory\"></span>          </span>\n",
      "                  </span></div><br><div class=\"comment\">\n",
      "                  <div class=\"commtext c00\">It’t that precisely embeddings API from OpenAI? It has all context so very useful for search</div>\n",
      "              <div class='reply'>        <p><font size=\"1\">\n",
      "                  </font>\n",
      "      </div></div></td></tr>\n",
      "        </table></td></tr>\n",
      "                <tr class='athing comtr' id='34426110'><td><table border='0'>  <tr>    <td class='ind' indent='1'><img src=\"s.gif\" height=\"1\" width=\"40\"></td><td valign=\"top\" class=\"votelinks\">\n",
      "      <center><a id='up_34426110' href='vote?id=34426110&amp;how=up&amp;goto=item%3Fid%3D34425779'><div class='votearrow' title='upvote'></div></a></center>    </td><td class=\"default\"><div style=\"margin-top:2px; margin-bottom:-10px;\"><span class=\"comhead\">\n",
      "          <a href=\"user?id=cs702\" class=\"hnuser\">cs702</a> <span class=\"age\" title=\"2023-01-18T12:57:36\"><a href=\"item?id=34426110\">on Jan 18, 2023</a></span> <span id=\"unv_34426110\"></span>          <span class='navs'>\n",
      "             | <a href=\"#34426044\" class=\"clicky\" aria-hidden=\"true\">parent</a> <a class=\"togg clicky\" id=\"34426110\" n=\"1\" href=\"javascript:void(0)\">[–]</a><span class=\"onstory\"></span>          </span>\n",
      "                  </span></div><br><div class=\"comment\">\n",
      "                  <div class=\"commtext c00\">Not quite. My understanding is that OpenAI&#x27;s various embeddings APIs return only a <i>single vector per document</i>, instead of the sequence of hidden states corresponding to each predicted next token in the response generated by a GPT-type LLM.<p>Imagine getting generated text from a GPT LLM that comes with a deep embedding of each generated token&#x27;s &quot;contextual meaning&quot;:<p><pre><code>  [(text_token, deep_emb), (text_token, deep_emb), ...]\n",
      "</code></pre>\n",
      "allowing higher-level models and apps to use all the information in those rich representations as inputs.</div>\n",
      "              <div class='reply'>        <p><font size=\"1\">\n",
      "                  </font>\n",
      "      </div></div></td></tr>\n",
      "        </table></td></tr>\n",
      "                  </table>\n",
      "  <br><br>\n",
      "</td></tr>\n",
      "<tr><td><img src=\"s.gif\" height=\"10\" width=\"0\"><table width=\"100%\" cellspacing=\"0\" cellpadding=\"1\"><tr><td bgcolor=\"#ff6600\"></td></tr></table><br>\n",
      "<center><span class=\"yclinks\"><a href=\"newsguidelines.html\">Guidelines</a> | <a href=\"newsfaq.html\">FAQ</a> | <a href=\"lists\">Lists</a> | <a href=\"https://github.com/HackerNews/API\">API</a> | <a href=\"security.html\">Security</a> | <a href=\"https://www.ycombinator.com/legal/\">Legal</a> | <a href=\"https://www.ycombinator.com/apply/\">Apply to YC</a> | <a href=\"mailto:hn@ycombinator.com\">Contact</a></span><br><br>\n",
      "<form method=\"get\" action=\"//hn.algolia.com/\">Search: <input type=\"text\" name=\"q\" size=\"17\" autocorrect=\"off\" spellcheck=\"false\" autocapitalize=\"off\" autocomplete=\"off\"></form></center></td></tr>      </table></center></body>\n",
      "      <script type='text/javascript' src='hn.js?rnO0aRh0nJpk4wNA6joY'></script>\n",
      "  </html>\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We should find the subject of the comments in the webpage.\n",
      "Action: google-search\n",
      "Action Input: GPT3 deep sequence embeddings API\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mJan 18, 2023 ... Imagine if OpenAI made GPT3's final hidden states available via an API (\"GPT3 deep sequence embeddings v1.0\"), next to each generated text ... Nov 9, 2023 ... ... GPT models use the last token in the sequence.) So the transformer ... A deep, intuitive understanding of text embeddings can help you ... Jan 14, 2023 ... Then you ask GPT-3 to answer the question based on all the top correlated facts in your prompt. This is probably the best way to extract ... Mar 10, 2023 ... ... GPT-3 model. Each layer applies a set of operations to the input sequence, transforming the embeddings at each step and gradually building ... Jun 27, 2023 ... embeddings, gpt-4, api · egils June 27, 2023, 3:22pm 1. Scenario: I have a chatbot with memory built on GPT-4. Each time user asks new question ... May 1, 2023 ... In order to use this in an LLM (large language model) like GPT, the LLM needs to know what it means. To do that, we can turn the word hamburger ... Embeddings. Learn how to turn text into numbers, unlocking use cases like search. New embedding models. 4 days ago ... Up to 4 sequences where the API will stop generating further tokens. ... Embeddings. HTTP Copy. POST https://{endpoint}/openai/deployments/{ ... Jan 5, 2021 ... ... GPT-2 and GPT-3. Although deep learning has revolutionized computer vision, current approaches have several major problems: typical vision ... 5 days ago ... gpt-35-turbo (1106-Preview), US Gov Virginia. Embeddings models. These models can only be used with Embedding API requests. Note. text-embedding ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The subject of the comments in the webpage is GPT-3 deep sequence embeddings API.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = my_agent(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subject of the comments in the webpage is GPT-3 deep sequence embeddings API.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XI-) Analisador de saída avançado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Defina a estrutura de saída que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.** Use field_validators para validar o formato de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suggestions_Output_Structure(BaseModel):\n",
    "    words: List[str] = Field(\n",
    "        description=\"list of substitute words based on the context\"\n",
    "    )\n",
    "    reasons: List[str] = Field(\n",
    "        description=\"the reasoning of why this word fits the context\"\n",
    "    )\n",
    "\n",
    "    #Throw error if the substitute word starts with a number\n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, info):\n",
    "        for item in info:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"ERROR: The word cannot start with a number\")\n",
    "        return info\n",
    "\n",
    "    @validator('reasons')\n",
    "    def end_with_dot(cls, info):\n",
    "      for idx, item in enumerate(info):\n",
    "        if item[-1] != \".\":\n",
    "          info[idx] += \".\"\n",
    "      return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3.** Crie o analisador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser = PydanticOutputParser(\n",
    "    pydantic_object=Suggestions_Output_Structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4.** Crie o modelo de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_template = \"\"\"\n",
    "Offer a list of suggestions to substitute the specified\n",
    "target_word based on the present context and the reasoning\n",
    "for each word.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate(\n",
    "    template=my_template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": my_parser.get_format_instructions()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5.** Determine a entrada do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = my_prompt.format_prompt(\n",
    "    target_word=\"loyalty\",\n",
    "    context=\"\"\"\n",
    "    The loyalty of the soldier was so great that\n",
    "    even under severe torture, he refused to betray\n",
    "    his comrades.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **6.** Aplique o analisador para obter a estrutura de saída desejada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions_Output_Structure(words=['devotion', 'faithfulness', 'allegiance', 'fidelity', 'dedication'], reasons=['These words all convey a strong sense of commitment and loyalty, similar to the original word.'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "output = llm(user_input.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
